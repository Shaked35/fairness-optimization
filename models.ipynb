{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd31d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from fairness_methods.methods import FairnessMethods\n",
    "from models.basic_mf_model import BasicMatrixFactorization\n",
    "from utils.data_generator import *\n",
    "from utils.metrics import RecommendationSystemMetrics\n",
    "from utils.util import *\n",
    "from models.nn_fairness_model import run_nn_fairness_model\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99daaab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = generate_real_data()\n",
    "# ml_data = ml_data.iloc[:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "896ec1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize user id\n",
    "ml_data[\"user_id\"] = ml_data[\"user_id\"].replace(\n",
    "    {user_id: index for (index, user_id) in enumerate(list(ml_data[\"user_id\"].unique()))})\n",
    "ml_data[\"movie_id\"] = ml_data[\"movie_id\"].replace(\n",
    "    {movie_id: index for (index, movie_id) in enumerate(list(ml_data[\"movie_id\"].unique()))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85dec41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_id_to_group(df):\n",
    "    \"\"\"\n",
    "    split movie id into groups types like genres\n",
    "    :param df: input data\n",
    "    :return: dict with movie_id as key group id as value\n",
    "    \"\"\"\n",
    "    movie_id_to_index = {movie_id: index for (index, movie_id) in enumerate(df['movie_id'].unique())}\n",
    "    movie_id_to_group = {}\n",
    "    for row in df[[\"movie_id\"] + GENRES].to_dict('records'):\n",
    "        # set genres group by women ratings and men ratings as wrote in the paper\n",
    "        # \"Women and men both score action, crime, and sci-fi films about equally, but men rate these film much more frequently\"\n",
    "        is_woman_group = 1 if sum([row[genre] for genre in WOMEN_GENRES]) > 0 else 0\n",
    "        movie_id_to_group[movie_id_to_index[row['movie_id']]] = is_woman_group\n",
    "\n",
    "    return movie_id_to_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9456ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ids_by_gender(df):\n",
    "    \"\"\"\n",
    "    split the input users to groups by gender\n",
    "    :param df: input data\n",
    "    :return: user_gender: gender to list of user ids, user_id_to_group: dict of user ids to group id\n",
    "    \"\"\"\n",
    "    user_id_to_index = {user_id: index for (index, user_id) in enumerate(df['user_id'].unique())}\n",
    "    user_gender = {}\n",
    "    user_id_to_group = {}\n",
    "    for row in df[[\"user_id\", \"gender\"]].to_dict('records'):\n",
    "        user_gender.setdefault(row['gender'], []).append(user_id_to_index[row['user_id']])\n",
    "        user_id_to_group[user_id_to_index[row['user_id']]] = 1 if row['gender'] == \"F\" else 0\n",
    "    user_gender[\"F\"] = list(set(user_gender[\"F\"]))\n",
    "    user_gender[\"M\"] = list(set(user_gender[\"M\"]))\n",
    "    return user_gender, user_id_to_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce065c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(tmp_df, zero_matrix, columns=\"movie_id\", test_ratio=0.2, movie_len=True):\n",
    "    \"\"\"\n",
    "    preprocess before training. split into train and test, fill empty values with zero matrix, add missing items and convert it to matrix\n",
    "    :param tmp_df: input data\n",
    "    :param zero_matrix: output matrix shape with zero values\n",
    "    :param columns: column name\n",
    "    :param test_ratio: percentage of test data\n",
    "    :param movie_len: is it movie_len dataset or synthetic dataset\n",
    "    :return: train_df, test_df, tmp_user_gender: gender to list of user ids, tmp_user_id_to_group: dict of user ids to group id dict with movie_id as key group id as value\n",
    "    \"\"\"\n",
    "    tmp_train, tmp_test = train_test_split(tmp_df, test_size=test_ratio, shuffle=True, stratify=tmp_df[[\"user_id\"]])\n",
    "#     tmp_train, tmp_test = train_test_split(tmp_df, test_size=test_ratio, shuffle=True)\n",
    "\n",
    "    tmp_train = tmp_train[tmp_train[columns].isin(tmp_test[columns].unique())]\n",
    "    tmp_user_gender, tmp_user_id_to_group = get_user_ids_by_gender(tmp_df)\n",
    "    item_to_group = get_item_id_to_group(tmp_df) if movie_len else {}\n",
    "    tmp_train_set = normalize_train_test_matrix(columns, tmp_train, zero_matrix)\n",
    "    tmp_test_set = normalize_train_test_matrix(columns, tmp_test, zero_matrix)\n",
    "    tmp_test_set = tmp_test_set.replace(0, np.nan)\n",
    "    print(\"train_set shape: \", tmp_train_set.shape, \" test_set shape: \", tmp_test_set.shape)\n",
    "    return tmp_train_set, tmp_test_set.to_numpy(), tmp_user_gender, tmp_user_id_to_group, item_to_group\n",
    "\n",
    "\n",
    "def normalize_train_test_matrix(columns, tmp_split_df, zero_matrix):\n",
    "    print(\"start normalization\")\n",
    "    tmp_set = pd.pivot_table(tmp_split_df, values='rating', index='user_id', columns=columns)\n",
    "    tmp_set = (tmp_set-tmp_set.min())/(tmp_set.max()-tmp_set.min())\n",
    "    for source_matrix_column in zero_matrix.columns:\n",
    "        if source_matrix_column not in tmp_set.columns:\n",
    "            tmp_set[source_matrix_column] = 0\n",
    "    tmp_set = tmp_set[zero_matrix.columns]\n",
    "    tmp_set.columns = range(tmp_set.shape[1])\n",
    "    tmp_set.fillna(0, inplace=True)\n",
    "    print(\"finished normalization\")\n",
    "    return tmp_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed60b775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start normalization\n",
      "finished normalization\n",
      "start normalization\n",
      "finished normalization\n",
      "train_set shape:  (4297, 1308)  test_set shape:  (4297, 1308)\n"
     ]
    }
   ],
   "source": [
    "empty_basic_matrix = pd.pivot_table(ml_data, values='rating', index='user_id', columns=\"movie_id\")\n",
    "empty_basic_matrix[:] = 0\n",
    "train_set, test_set, user_gender, user_id_to_group, item_to_group = preprocessing(ml_data,empty_basic_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd5f80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c76567",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.03619726374745369\n",
      "Epoch 1, Loss: 0.03529508784413338\n",
      "Epoch 2, Loss: 0.03456813469529152\n",
      "Epoch 3, Loss: 0.03401249274611473\n",
      "Epoch 4, Loss: 0.03360583260655403\n",
      "Epoch 5, Loss: 0.03331625461578369\n",
      "Epoch 6, Loss: 0.033109888434410095\n",
      "Epoch 7, Loss: 0.03295747935771942\n",
      "Epoch 8, Loss: 0.032837286591529846\n",
      "Epoch 9, Loss: 0.03273520618677139\n",
      "Epoch 10, Loss: 0.032642606645822525\n",
      "Epoch 11, Loss: 0.032554201781749725\n",
      "Epoch 12, Loss: 0.03246687352657318\n",
      "Epoch 13, Loss: 0.03237871080636978\n",
      "Epoch 14, Loss: 0.03228846937417984\n",
      "Epoch 15, Loss: 0.03219551220536232\n",
      "Epoch 16, Loss: 0.03209982439875603\n",
      "Epoch 17, Loss: 0.032002173364162445\n",
      "Epoch 18, Loss: 0.03190397843718529\n",
      "Epoch 19, Loss: 0.031807154417037964\n",
      "Epoch 20, Loss: 0.031714048236608505\n",
      "Epoch 21, Loss: 0.03162704035639763\n",
      "Epoch 22, Loss: 0.03154800087213516\n",
      "Epoch 23, Loss: 0.03147796168923378\n",
      "Epoch 24, Loss: 0.03141707554459572\n",
      "Epoch 25, Loss: 0.03136459365487099\n",
      "Epoch 26, Loss: 0.03131904453039169\n",
      "Epoch 27, Loss: 0.03127823397517204\n",
      "Epoch 28, Loss: 0.031239410862326622\n",
      "Epoch 29, Loss: 0.031199760735034943\n",
      "Epoch 30, Loss: 0.031156571581959724\n",
      "Epoch 31, Loss: 0.031107628718018532\n",
      "Epoch 32, Loss: 0.03105146251618862\n",
      "Epoch 33, Loss: 0.030987422913312912\n",
      "Epoch 34, Loss: 0.030915532261133194\n",
      "Epoch 35, Loss: 0.030836300924420357\n",
      "Epoch 36, Loss: 0.030750468373298645\n",
      "Epoch 37, Loss: 0.030658788979053497\n",
      "Epoch 38, Loss: 0.03056204691529274\n",
      "Epoch 39, Loss: 0.030461138114333153\n",
      "Epoch 40, Loss: 0.030357135459780693\n",
      "Epoch 41, Loss: 0.030251381918787956\n",
      "Epoch 42, Loss: 0.03014535829424858\n",
      "Epoch 43, Loss: 0.030040567740797997\n",
      "Epoch 44, Loss: 0.029938070103526115\n",
      "Epoch 45, Loss: 0.0298384428024292\n",
      "Epoch 46, Loss: 0.02974196895956993\n",
      "Epoch 47, Loss: 0.02964874729514122\n",
      "Epoch 48, Loss: 0.029558803886175156\n",
      "Epoch 49, Loss: 0.02947234734892845\n",
      "Epoch 50, Loss: 0.029389463365077972\n",
      "Epoch 51, Loss: 0.02931027300655842\n",
      "Epoch 52, Loss: 0.029234696179628372\n",
      "Epoch 53, Loss: 0.029162699356675148\n",
      "Epoch 54, Loss: 0.029094185680150986\n",
      "Epoch 55, Loss: 0.029028983786702156\n",
      "Epoch 56, Loss: 0.028966911137104034\n",
      "Epoch 57, Loss: 0.028907842934131622\n",
      "Epoch 58, Loss: 0.028851594775915146\n",
      "Epoch 59, Loss: 0.02879803627729416\n",
      "Epoch 60, Loss: 0.02874695509672165\n",
      "Epoch 61, Loss: 0.02869807370007038\n",
      "Epoch 62, Loss: 0.02865121141076088\n",
      "Epoch 63, Loss: 0.028606154024600983\n",
      "Epoch 64, Loss: 0.02856258861720562\n",
      "Epoch 65, Loss: 0.02852020040154457\n",
      "Epoch 66, Loss: 0.028478756546974182\n",
      "Epoch 67, Loss: 0.02843809686601162\n",
      "Epoch 68, Loss: 0.028398212045431137\n",
      "Epoch 69, Loss: 0.028359146788716316\n",
      "Epoch 70, Loss: 0.02832108736038208\n",
      "Epoch 71, Loss: 0.028284287080168724\n",
      "Epoch 72, Loss: 0.028249021619558334\n",
      "Epoch 73, Loss: 0.028215495869517326\n",
      "Epoch 74, Loss: 0.02818393148481846\n",
      "Epoch 75, Loss: 0.028154464438557625\n",
      "Epoch 76, Loss: 0.02812710404396057\n",
      "Epoch 77, Loss: 0.028101816773414612\n",
      "Epoch 78, Loss: 0.02807854488492012\n",
      "Epoch 79, Loss: 0.028057176619768143\n",
      "Epoch 80, Loss: 0.02803763560950756\n",
      "Epoch 81, Loss: 0.028019849210977554\n",
      "Epoch 82, Loss: 0.028003720566630363\n",
      "Epoch 83, Loss: 0.02798912115395069\n",
      "Epoch 84, Loss: 0.027975931763648987\n",
      "Epoch 85, Loss: 0.027963988482952118\n",
      "Epoch 86, Loss: 0.027953121811151505\n",
      "Epoch 87, Loss: 0.027943171560764313\n",
      "Epoch 88, Loss: 0.027933964505791664\n",
      "Epoch 89, Loss: 0.027925342321395874\n",
      "Epoch 90, Loss: 0.02791718766093254\n",
      "Epoch 91, Loss: 0.0279094185680151\n",
      "Epoch 92, Loss: 0.027901986613869667\n",
      "Epoch 93, Loss: 0.027894865721464157\n",
      "Epoch 94, Loss: 0.02788803167641163\n",
      "Epoch 95, Loss: 0.027881478890776634\n",
      "Epoch 96, Loss: 0.027875211089849472\n",
      "Epoch 97, Loss: 0.027869239449501038\n",
      "Epoch 98, Loss: 0.02786356396973133\n",
      "Epoch 99, Loss: 0.027858175337314606\n",
      "Epoch 100, Loss: 0.027853067964315414\n",
      "Epoch 101, Loss: 0.02784821018576622\n",
      "Epoch 102, Loss: 0.027843590825796127\n",
      "Epoch 103, Loss: 0.02783917635679245\n",
      "Epoch 104, Loss: 0.027834929525852203\n",
      "Epoch 105, Loss: 0.027830833569169044\n",
      "Epoch 106, Loss: 0.027826882898807526\n",
      "Epoch 107, Loss: 0.027823064476251602\n",
      "Epoch 108, Loss: 0.027819372713565826\n",
      "Epoch 109, Loss: 0.027815796434879303\n",
      "Epoch 110, Loss: 0.02781229466199875\n",
      "Epoch 111, Loss: 0.027808837592601776\n",
      "Epoch 112, Loss: 0.02780541218817234\n",
      "Epoch 113, Loss: 0.02780201844871044\n",
      "Epoch 114, Loss: 0.027798673138022423\n",
      "Epoch 115, Loss: 0.027795400470495224\n",
      "Epoch 116, Loss: 0.027792207896709442\n",
      "Epoch 117, Loss: 0.027789073064923286\n",
      "Epoch 118, Loss: 0.027785977348685265\n",
      "Epoch 119, Loss: 0.027782917022705078\n",
      "Epoch 120, Loss: 0.02777988463640213\n",
      "Epoch 121, Loss: 0.02777690254151821\n",
      "Epoch 122, Loss: 0.027773961424827576\n",
      "Epoch 123, Loss: 0.02777104079723358\n",
      "Epoch 124, Loss: 0.02776811644434929\n",
      "Epoch 125, Loss: 0.027765175327658653\n",
      "Epoch 126, Loss: 0.02776223234832287\n",
      "Epoch 127, Loss: 0.02775929495692253\n",
      "Epoch 128, Loss: 0.027756385505199432\n",
      "Epoch 129, Loss: 0.027753517031669617\n",
      "Epoch 130, Loss: 0.027750687673687935\n",
      "Epoch 131, Loss: 0.027747882530093193\n",
      "Epoch 132, Loss: 0.027745092287659645\n",
      "Epoch 133, Loss: 0.027742311358451843\n",
      "Epoch 134, Loss: 0.027739521116018295\n",
      "Epoch 135, Loss: 0.027736729010939598\n",
      "Epoch 136, Loss: 0.027733927592635155\n",
      "Epoch 137, Loss: 0.027731114998459816\n",
      "Epoch 138, Loss: 0.02772829867899418\n",
      "Epoch 139, Loss: 0.027725478634238243\n",
      "Epoch 140, Loss: 0.027722641825675964\n",
      "Epoch 141, Loss: 0.027719786390662193\n",
      "Epoch 142, Loss: 0.027716921642422676\n",
      "Epoch 143, Loss: 0.027714040130376816\n",
      "Epoch 144, Loss: 0.02771114557981491\n",
      "Epoch 145, Loss: 0.02770824544131756\n",
      "Epoch 146, Loss: 0.027705347165465355\n",
      "Epoch 147, Loss: 0.027702460065484047\n",
      "Epoch 148, Loss: 0.027699587866663933\n",
      "Epoch 149, Loss: 0.02769673801958561\n",
      "Epoch 150, Loss: 0.027693897485733032\n",
      "Epoch 151, Loss: 0.02769104391336441\n",
      "Epoch 152, Loss: 0.0276881605386734\n",
      "Epoch 153, Loss: 0.027685243636369705\n",
      "Epoch 154, Loss: 0.027682311832904816\n",
      "Epoch 155, Loss: 0.027679376304149628\n",
      "Epoch 156, Loss: 0.027676435187458992\n",
      "Epoch 157, Loss: 0.027673492208123207\n",
      "Epoch 158, Loss: 0.027670523151755333\n",
      "Epoch 159, Loss: 0.027667546644806862\n",
      "Epoch 160, Loss: 0.027664557099342346\n",
      "Epoch 161, Loss: 0.027661548927426338\n",
      "Epoch 162, Loss: 0.027658523991703987\n",
      "Epoch 163, Loss: 0.02765546925365925\n",
      "Epoch 164, Loss: 0.02765238843858242\n",
      "Epoch 165, Loss: 0.027649275958538055\n",
      "Epoch 166, Loss: 0.027646135538816452\n",
      "Epoch 167, Loss: 0.02764296717941761\n",
      "Epoch 168, Loss: 0.02763977088034153\n",
      "Epoch 169, Loss: 0.02763654664158821\n",
      "Epoch 170, Loss: 0.02763330191373825\n",
      "Epoch 171, Loss: 0.027630023658275604\n",
      "Epoch 172, Loss: 0.02762671932578087\n",
      "Epoch 173, Loss: 0.027623379603028297\n",
      "Epoch 174, Loss: 0.02762000262737274\n",
      "Epoch 175, Loss: 0.027616597712039948\n",
      "Epoch 176, Loss: 0.027613185346126556\n",
      "Epoch 177, Loss: 0.027609772980213165\n",
      "Epoch 178, Loss: 0.027606358751654625\n",
      "Epoch 179, Loss: 0.027602924033999443\n",
      "Epoch 180, Loss: 0.02759944275021553\n",
      "Epoch 181, Loss: 0.027595899999141693\n",
      "Epoch 182, Loss: 0.027592305094003677\n",
      "Epoch 183, Loss: 0.027588676661252975\n",
      "Epoch 184, Loss: 0.027585025876760483\n",
      "Epoch 185, Loss: 0.027581363916397095\n",
      "Epoch 186, Loss: 0.027577688917517662\n",
      "Epoch 187, Loss: 0.02757399156689644\n",
      "Epoch 188, Loss: 0.027570273727178574\n",
      "Epoch 189, Loss: 0.02756652794778347\n",
      "Epoch 190, Loss: 0.027562737464904785\n",
      "Epoch 191, Loss: 0.02755889669060707\n",
      "Epoch 192, Loss: 0.027555011212825775\n",
      "Epoch 193, Loss: 0.027551084756851196\n",
      "Epoch 194, Loss: 0.027547121047973633\n",
      "Epoch 195, Loss: 0.02754313312470913\n",
      "Epoch 196, Loss: 0.027539117261767387\n",
      "Epoch 197, Loss: 0.02753506973385811\n",
      "Epoch 198, Loss: 0.027530981227755547\n",
      "Epoch 199, Loss: 0.027526848018169403\n",
      "{'model_type': 'nmf', 'fit_time': 1208, 'Loss_arr': [0.036197264, 0.035295088, 0.034568135, 0.034012493, 0.033605833, 0.033316255, 0.03310989, 0.03295748, 0.032837287, 0.032735206, 0.032642607, 0.0325542, 0.032466874, 0.03237871, 0.03228847, 0.032195512, 0.032099824, 0.032002173, 0.03190398, 0.031807154, 0.03171405, 0.03162704, 0.031548, 0.03147796, 0.031417076, 0.031364594, 0.031319045, 0.031278234, 0.03123941, 0.03119976, 0.031156572, 0.031107629, 0.031051463, 0.030987423, 0.030915532, 0.0308363, 0.030750468, 0.030658789, 0.030562047, 0.030461138, 0.030357135, 0.030251382, 0.030145358, 0.030040568, 0.02993807, 0.029838443, 0.029741969, 0.029648747, 0.029558804, 0.029472347, 0.029389463, 0.029310273, 0.029234696, 0.0291627, 0.029094186, 0.029028984, 0.028966911, 0.028907843, 0.028851595, 0.028798036, 0.028746955, 0.028698074, 0.028651211, 0.028606154, 0.028562589, 0.0285202, 0.028478757, 0.028438097, 0.028398212, 0.028359147, 0.028321087, 0.028284287, 0.028249022, 0.028215496, 0.028183931, 0.028154464, 0.028127104, 0.028101817, 0.028078545, 0.028057177, 0.028037636, 0.02801985, 0.02800372, 0.027989121, 0.027975932, 0.027963988, 0.027953122, 0.027943172, 0.027933965, 0.027925342, 0.027917188, 0.027909419, 0.027901987, 0.027894866, 0.027888032, 0.027881479, 0.027875211, 0.02786924, 0.027863564, 0.027858175, 0.027853068, 0.02784821, 0.02784359, 0.027839176, 0.02783493, 0.027830834, 0.027826883, 0.027823064, 0.027819373, 0.027815796, 0.027812295, 0.027808838, 0.027805412, 0.027802018, 0.027798673, 0.0277954, 0.027792208, 0.027789073, 0.027785977, 0.027782917, 0.027779885, 0.027776903, 0.027773961, 0.02777104, 0.027768116, 0.027765175, 0.027762232, 0.027759295, 0.027756386, 0.027753517, 0.027750688, 0.027747883, 0.027745092, 0.027742311, 0.027739521, 0.027736729, 0.027733928, 0.027731115, 0.027728299, 0.027725479, 0.027722642, 0.027719786, 0.027716922, 0.02771404, 0.027711146, 0.027708245, 0.027705347, 0.02770246, 0.027699588, 0.027696738, 0.027693897, 0.027691044, 0.02768816, 0.027685244, 0.027682312, 0.027679376, 0.027676435, 0.027673492, 0.027670523, 0.027667547, 0.027664557, 0.027661549, 0.027658524, 0.02765547, 0.027652388, 0.027649276, 0.027646136, 0.027642967, 0.02763977, 0.027636547, 0.027633302, 0.027630024, 0.02762672, 0.02762338, 0.027620003, 0.027616598, 0.027613185, 0.027609773, 0.027606359, 0.027602924, 0.027599443, 0.0275959, 0.027592305, 0.027588677, 0.027585026, 0.027581364, 0.027577689, 0.027573992, 0.027570274, 0.027566528, 0.027562737, 0.027558897, 0.027555011, 0.027551085, 0.027547121, 0.027543133, 0.027539117, 0.02753507, 0.027530981, 0.027526848], 'metric': None, 'lr': 0.001, 'early_stop': 1e-08, 'embed_dim': 8, 'layers': [64, 32, 16, 8], 'model': <keras.engine.functional.Functional object at 0x000001EFE49424C0>, 'val_score_unfair_arr': [], 'val_score_mse_loss_arr': [0.73916155, 0.7326582, 0.72691673, 0.72211266, 0.71828496, 0.7153351, 0.713067, 0.71126837, 0.7097523, 0.7083833, 0.7070746, 0.70576346, 0.7044048, 0.7029625, 0.7014083, 0.69971746, 0.69787365, 0.69586843, 0.693704, 0.69139665, 0.688974, 0.68647313, 0.68393266, 0.68138915, 0.6788807, 0.6764394, 0.674097, 0.6718814, 0.66981584, 0.66792244, 0.66621804, 0.66471124, 0.66340095, 0.6622768, 0.66131866, 0.66049695, 0.65977573, 0.6591151, 0.65847415, 0.6578127, 0.65709597, 0.6562913, 0.6553655, 0.6542905, 0.6530422, 0.65160453, 0.6499717, 0.64815027, 0.6461569, 0.6440124, 0.64174473, 0.63938165, 0.63695174, 0.63448566, 0.6320155, 0.6295787, 0.62721205, 0.6249542, 0.62283915, 0.6208966, 0.61914575, 0.61759484, 0.61623865, 0.6150652, 0.61405444, 0.61318225, 0.61242306, 0.61175096, 0.61114436, 0.6105862, 0.6100637, 0.6095706, 0.60910237, 0.6086585, 0.6082408, 0.607852, 0.6074936, 0.60716903, 0.6068783, 0.60662097, 0.6063956, 0.60619974, 0.60602975, 0.60588115, 0.6057496, 0.6056321, 0.60552734, 0.6054345, 0.6053542, 0.60528815, 0.60523784, 0.605205, 0.60518986, 0.60519284, 0.605213, 0.6052473, 0.6052941, 0.6053485, 0.6054071, 0.60546565, 0.605521, 0.6055703, 0.6056118, 0.6056454, 0.605672, 0.60569215, 0.60570765, 0.60572004, 0.6057313, 0.60574347, 0.6057573, 0.6057739, 0.605793, 0.6058125, 0.6058308, 0.60584795, 0.6058633, 0.6058762, 0.6058864, 0.6058934, 0.60589755, 0.60589904, 0.60589814, 0.6058949, 0.6058901, 0.6058831, 0.6058735, 0.60586244, 0.60585034, 0.60583806, 0.60582745, 0.60582036, 0.6058178, 0.60582054, 0.60582817, 0.6058395, 0.6058534, 0.60586876, 0.6058836, 0.60589737, 0.60590893, 0.60591793, 0.6059237, 0.6059259, 0.60592574, 0.6059237, 0.6059211, 0.60591894, 0.60591835, 0.6059203, 0.6059255, 0.60593414, 0.60594547, 0.6059585, 0.6059719, 0.6059835, 0.60599303, 0.60599947, 0.60600364, 0.60600597, 0.6060078, 0.60600895, 0.60601026, 0.60601157, 0.6060139, 0.6060172, 0.6060216, 0.60602784, 0.6060347, 0.6060413, 0.6060491, 0.60605675, 0.60606444, 0.606072, 0.6060782, 0.60608274, 0.60608524, 0.60608745, 0.6060896, 0.60609394, 0.60610145, 0.6061123, 0.6061246, 0.60613716, 0.6061479, 0.6061555, 0.6061591, 0.6061613, 0.6061626, 0.6061653, 0.6061706, 0.6061786, 0.6061893, 0.60620195, 0.6062148, 0.6062259, 0.60623527, 0.60624343, 0.6062508, 0.60625964], 'train_score_mse_loss_arr': [0.19025576, 0.18786988, 0.18592507, 0.18442476, 0.18331897, 0.18252744, 0.18196122, 0.18154192, 0.18121064, 0.18092881, 0.18067262, 0.18042777, 0.18018569, 0.17994086, 0.17968994, 0.17943116, 0.17916423, 0.17889144, 0.17861688, 0.17834562, 0.17808434, 0.1778399, 0.17761758, 0.17742026, 0.17724864, 0.17710046, 0.17697184, 0.17685647, 0.17674671, 0.17663458, 0.17651224, 0.1763735, 0.17621428, 0.1760325, 0.17582805, 0.1756027, 0.17535806, 0.17509659, 0.17482011, 0.17453128, 0.174233, 0.17392917, 0.17362417, 0.17332219, 0.17302625, 0.17273808, 0.17245863, 0.17218815, 0.17192674, 0.17167512, 0.17143354, 0.17120242, 0.17098159, 0.17077093, 0.17057015, 0.17037891, 0.17019667, 0.17002301, 0.1698576, 0.16969986, 0.16954923, 0.16940501, 0.16926673, 0.16913351, 0.16900463, 0.16887929, 0.16875647, 0.16863598, 0.16851768, 0.1684017, 0.16828866, 0.16817927, 0.16807449, 0.1679747, 0.1678807, 0.16779293, 0.16771133, 0.16763596, 0.16756657, 0.16750273, 0.1674444, 0.16739127, 0.16734314, 0.16729948, 0.16726011, 0.16722438, 0.16719191, 0.16716214, 0.16713454, 0.16710876, 0.1670844, 0.16706112, 0.16703884, 0.1670176, 0.1669971, 0.16697751, 0.16695866, 0.16694082, 0.16692384, 0.16690773, 0.16689236, 0.16687782, 0.16686402, 0.16685084, 0.16683806, 0.16682565, 0.16681392, 0.1668024, 0.16679142, 0.16678075, 0.16677016, 0.1667599, 0.16674952, 0.16673936, 0.16672927, 0.16671957, 0.16670994, 0.16670059, 0.16669124, 0.16668212, 0.16667297, 0.16666406, 0.16665526, 0.16664647, 0.16663767, 0.16662882, 0.16662002, 0.16661122, 0.16660245, 0.16659386, 0.16658537, 0.16657698, 0.16656855, 0.16656025, 0.16655189, 0.16654345, 0.1665351, 0.16652663, 0.1665182, 0.16650972, 0.16650118, 0.16649261, 0.16648406, 0.16647533, 0.1664666, 0.16645794, 0.1664492, 0.16644058, 0.16643189, 0.16642337, 0.16641486, 0.16640629, 0.1663976, 0.16638888, 0.16638003, 0.16637115, 0.16636235, 0.1663535, 0.16634463, 0.16633563, 0.16632669, 0.1663176, 0.16630857, 0.16629937, 0.16629007, 0.1662807, 0.16627128, 0.16626176, 0.16625215, 0.1662425, 0.16623265, 0.1662228, 0.16621287, 0.16620281, 0.1661927, 0.16618243, 0.16617216, 0.16616188, 0.1661516, 0.16614124, 0.16613086, 0.16612011, 0.16610935, 0.1660984, 0.16608742, 0.1660764, 0.16606538, 0.1660542, 0.16604304, 0.16603164, 0.16602027, 0.16600867, 0.16599709, 0.1659852, 0.16597326, 0.16596124, 0.16594912, 0.16593696, 0.16592465, 0.16591214], 'train_score_unfair_arr': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]}\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\concatenate\n",
      "......vars\n",
      "...layers\\concatenate_1\n",
      "......vars\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_3\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\embedding\n",
      "......vars\n",
      ".........0\n",
      "...layers\\embedding_1\n",
      "......vars\n",
      ".........0\n",
      "...layers\\embedding_2\n",
      "......vars\n",
      ".........0\n",
      "...layers\\embedding_3\n",
      "......vars\n",
      ".........0\n",
      "...layers\\flatten\n",
      "......vars\n",
      "...layers\\flatten_1\n",
      "......vars\n",
      "...layers\\flatten_2\n",
      "......vars\n",
      "...layers\\flatten_3\n",
      "......vars\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...layers\\input_layer_1\n",
      "......vars\n",
      "...layers\\multiply\n",
      "......vars\n",
      "...vars\n",
      "Keras model archive saving:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-02-26 21:09:41         6101\n",
      "metadata.json                                  2023-02-26 21:09:41           64\n",
      "variables.h5                                   2023-02-26 21:09:41       947648\n",
      "Epoch 0, Loss: 0.04433306306600571\n",
      "Epoch 1, Loss: 0.04383361339569092\n",
      "Epoch 2, Loss: 0.0433950200676918\n",
      "Epoch 3, Loss: 0.04298018664121628\n",
      "Epoch 4, Loss: 0.04256162792444229\n",
      "Epoch 5, Loss: 0.04212784767150879\n",
      "Epoch 6, Loss: 0.04167765751481056\n",
      "Epoch 7, Loss: 0.04121653735637665\n",
      "Epoch 8, Loss: 0.04075255244970322\n",
      "Epoch 9, Loss: 0.040293995290994644\n",
      "Epoch 10, Loss: 0.03984817862510681\n",
      "Epoch 11, Loss: 0.039420854300260544\n",
      "Epoch 12, Loss: 0.03901622071862221\n",
      "Epoch 13, Loss: 0.038637615740299225\n",
      "Epoch 14, Loss: 0.03828778490424156\n",
      "Epoch 15, Loss: 0.03796924650669098\n",
      "Epoch 16, Loss: 0.037684403359889984\n",
      "Epoch 17, Loss: 0.03743534907698631\n",
      "Epoch 18, Loss: 0.03722365200519562\n",
      "Epoch 19, Loss: 0.03704993426799774\n"
     ]
    }
   ],
   "source": [
    "for metric in [None,'val_score','abs_score','over_score','under_score']:\n",
    "    trainig_dict= run_nn_fairness_model('nmf',train_set,test_set,metric = metric,num_epochs=200, num_users=train_set.shape[0], num_items=train_set.shape[1], embed_dim=8,\n",
    "                          dis_group=user_gender['F'], adv_group=user_gender['M'], lr=0.001,layers=[64,32,16,8], early_stop=0.00000001,unfairness_reg=0.5,early_stop_tol=3)\n",
    "    print(trainig_dict)\n",
    "    with open(f'training_dict_{metric}_Exp2.pickle', 'wb') as handle:\n",
    "        pickle.dump(trainig_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd24f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [None,'val_score','abs_score','over_score','under_score']:\n",
    "    \n",
    "    with open(f'trainig_dict_{metric}_exp_1.pickle', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "        print(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aebf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainig_dict_None_exp_1.pickle\n",
    "training_dict_None_exp_1.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c47e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'hello': 'world'}\n",
    "\n",
    "with open('training_dict+.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('filename.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print(a == b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}