{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b688b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict = []\n",
    "mypath = 'artifacts/best_movielens'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for file in onlyfiles:\n",
    "    with open(os.path.join(mypath,file), 'rb') as handle:\n",
    "            train_dict = pickle.load(handle)\n",
    "            list_dict.append(train_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc13c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for train_dict in list_dict:\n",
    "#     train_dict['mse_valid'] = train_dict[ 'val_score_mse_loss_arr'][-1]\n",
    "#     train_dict['mse_train'] = train_dict[ 'train_score_mse_loss_arr'][-1]\n",
    "#     train_dict['unfair_score'] = train_dict[ 'train_score_unfair_arr'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_nmf = {}\n",
    "for key in list(train_dict['val_dict_unfair']):\n",
    "    key_list = []\n",
    "    for train_dict in list_dict:\n",
    "        if train_dict['model_type'] == 'nmf':\n",
    "            key_list.append(train_dict['val_dict_unfair'][key])\n",
    "        key_dict = {key: key_list}\n",
    "        keys_nmf={**keys_nmf,**key_dict}\n",
    "\n",
    "keys_gmf = {}\n",
    "for key in list(train_dict['val_dict_unfair']):\n",
    "    key_list = []\n",
    "    for train_dict in list_dict:\n",
    "        if train_dict['model_type'] == 'gmf':\n",
    "            key_list.append(train_dict['val_dict_unfair'][key])\n",
    "        key_dict = {key: key_list}\n",
    "        keys_gmf={**keys_gmf,**key_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73881283",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hp = pd.DataFrame.from_dict(keys,orient='columns')[['val_score','abs_score','under_score','non_parity_score','rmse']]\n",
    "hp['model'] = hp.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tuning(tuning_data:pd.DataFrame, hyperparam_cols:str ,metric_cols:list ,model_col = 'model_type'):\n",
    "    hp_results_melt = tuning_data.melt(id_vars=[model_col]+ hyperparam_cols, value_vars=metric_cols)\n",
    "    for i, hyperparam_col in enumerate(hyperparam_cols):\n",
    "        hp_results_melt = hp_results_melt.sort_values(by =hyperparam_col)\n",
    "        hp_results_melt[hyperparam_col] = hp_results_melt[hyperparam_col].astype('str')\n",
    "        fig = px.box(hp_results_melt, x=hyperparam_col, y='value',  color='model_type', facet_col='variable',facet_col_wrap=2, title= hyperparam_col)\n",
    "        fig.update_yaxes(matches=None)\n",
    "        fig.update_yaxes(showticklabels=True)\n",
    "        fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb6d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tuning(tuning_data=hp, metric_cols= ['mse_train','mse_valid','unfair_score','fit_time'] , hyperparam_cols=['metric','embed_dim','lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33069871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_comparison(best_models:pd.DataFrame,metric_cols:list,time_col:str= None):\n",
    "    best_models['sum'] = best_models[metric_cols].abs().sum(axis=1)\n",
    "    best_models=best_models.sort_values(by='sum')\n",
    "    best_models['model']=best_models['model'].astype('str')\n",
    "    fig = px.bar(best_models,x='model', y=metric_cols, text_auto=True, opacity=0.7, title = 'metric comparison')\n",
    "    fig.show()\n",
    "    if time_col:\n",
    "\n",
    "        fig = px.bar(best_models,x='model', y=time_col, text_auto=True, opacity=0.7, title = 'fit time (sec)')\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5388e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_comparison(hp,['mse_train','mse_valid','unfair_score'],'fit_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d4ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gmf = pd.read_csv('res_gmf_30e_val.csv')\n",
    "df_gmf = df_gmf.rename({'Unnamed: 0':'metric','Unnamed: 6':'rmse' },axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366852e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['val_score','abs_score','over_score','under_score','non_parity_score','rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ac155",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for test in df_gmf.test.unique():\n",
    "    cur_df = df_gmf[df_gmf.test ==test]    \n",
    "    fig = ff.create_annotated_heatmap(z=cur_df[metrics].round(5).values, x=metrics, y=metrics, colorscale='Geyser', zmin=0, zmax=0.5,\n",
    "                                      showscale=True,opacity=0.9)  # redgreen platte\n",
    "    fig.update_layout(width=600, height=350, xaxis_nticks=36, title=test)\n",
    "    fig['layout']['xaxis']['autorange'] = \"reversed\"\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df = df_gmf[df_gmf.test ==test] \n",
    "sum_df[metrics] =0\n",
    "for test in df_gmf.test.unique():\n",
    "    cur_df = df_gmf[df_gmf.test ==test] \n",
    "    sum_df[metrics] =sum_df[metrics] + cur_df[metrics].values\n",
    "    sum_df[metrics] = sum_df[metrics]/4\n",
    "    \n",
    "fig = ff.create_annotated_heatmap(z=sum_df[metrics].round(5).values, x=metrics, y=metrics, colorscale='Geyser', zmin=0, zmax=0.2,\n",
    "                                  showscale=True,opacity=0.9)  # redgreen platte\n",
    "fig.update_layout(width=600, height=350, xaxis_nticks=36, title='mean')\n",
    "fig['layout']['xaxis']['autorange'] = \"reversed\"\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40618d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_df = pd.read_csv('tuning_syn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3951dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_df =tune_df.rename({'Unnamed: 0': 'objective','Unnamed: 10':'mse'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e909c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics =  ['val_score', 'abs_score', 'over_score', 'under_score', 'non_parity_score', 'mse']\n",
    "tune_params = ['objective','model','unfairness_reg','lr','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe99087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune_df_copy = tune_df.copy()\n",
    "# tune_df_copy['model'] = tune_df_copy.index\n",
    "# plot_metric_comparison(tune_df_copy,metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_df['lr']=tune_df['lr'].round(1)\n",
    "tune_df['unfairness_reg']=tune_df['unfairness_reg'].round(1)\n",
    "tune_df['early_stop']=tune_df_copy['early_stop'].round(6)\n",
    "\n",
    "tune_df['model_type']= tune_df['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865a6e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_tuning(tuning_data=tune_df, metric_cols= metrics , hyperparam_cols=['unfairness_reg','objective','lr','model_type','early_stop'],model_col ='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584f48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}